{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load load-movies.py\n",
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv('./data/tmdb_5000_movies.csv')\n",
    "credits = pd.read_csv('./data/tmdb_5000_credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load clean-movies.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# merge two datasets\n",
    "movies = pd.merge(left=movies, right=credits, left_on='id', right_on='movie_id')\n",
    "\n",
    "# turn json into python objects\n",
    "json_columns = ['genres', 'keywords', 'production_countries', 'spoken_languages', 'cast', 'crew']\n",
    "for column in json_columns:\n",
    "  movies[column] = movies[column].apply(json.loads, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "######## Actors ########\n",
    "\n",
    "# create functions that will help with extracting actor information\n",
    "def actor_to_id_string(actor):\n",
    "  return '{} - {}'.format(actor['name'], actor['id'])\n",
    "\n",
    "def cast_to_actors(cast):\n",
    "  actors = []\n",
    "  for person in cast:\n",
    "    actors.append(actor_to_id_string(person))\n",
    "  return actors\n",
    "\n",
    "# Create a new column (array) from cast\n",
    "movies.actors = movies.cast.apply(cast_to_actors)\n",
    "\n",
    "# Create a list of all actors\n",
    "all_actors = []\n",
    "for actors in movies.actors:\n",
    "  for actor in actors:\n",
    "    all_actors.append(actor)\n",
    "actors = pd.Series(all_actors)\n",
    "unique_actors_count = pd.crosstab(actors, columns='count')\n",
    "\n",
    "# Create function to determine if actor has been in more than 3 films\n",
    "actors_string_id_by_id = {}\n",
    "actor_counts_by_string_id = {}\n",
    "unique_actors_count_dict = unique_actors_count.to_dict()['count']\n",
    "\n",
    "def actor_has_more_than_x_movies(actor, number_of_movies = 3):\n",
    "    string_id = str(actor['id'])\n",
    "\n",
    "    # First, get the unique id created for the actor\n",
    "    actor_string_id = ''\n",
    "    if string_id in actors_string_id_by_id:\n",
    "        actor_string_id = actors_string_id_by_id[string_id]\n",
    "    else:\n",
    "        actor_string_id = actor_to_id_string(actor)\n",
    "        actors_string_id_by_id[string_id] = actor_string_id\n",
    "    \n",
    "    # Now let's see how many movies this actor has played in\n",
    "    actor_count = unique_actors_count_dict[actor_string_id]\n",
    "    return actor_count > number_of_movies\n",
    "\n",
    "# Create a list of all actors that have been in more than 3 movies\n",
    "movies_actors_ids = []\n",
    "for actors in movies.cast:\n",
    "  movie_actors_ids = []\n",
    "  for actor in actors:\n",
    "    if (not actor_has_more_than_x_movies(actor, 3)):\n",
    "      continue\n",
    "    movie_actors_ids.append(str(actor['id']))\n",
    "  movies_actors_ids.append(' '.join(movie_actors_ids))\n",
    "\n",
    "# Vectorize the list of actors\n",
    "def get_actor_feature_name(id):\n",
    "  return actors_string_id_by_id[id] + ' (actor)'\n",
    "vectorizer = CountVectorizer()\n",
    "movie_vector = vectorizer.fit_transform(movies_actors_ids).toarray()\n",
    "\n",
    "actor_feature_ids = vectorizer.get_feature_names()\n",
    "actor_feature_names = []\n",
    "for id in actor_feature_ids:\n",
    "  actor_name = get_actor_feature_name(id)\n",
    "  actor_feature_names.append(actor_name)\n",
    "\n",
    "actor_vector_works = pd.DataFrame(movie_vector, columns=actor_feature_names)\n",
    "actor_vector_works['id'] = movies.id\n",
    "\n",
    "# Create the label for whether Samuel L Jackson was in the film (samuel)\n",
    "movies['samuel'] = actor_vector_works[get_actor_feature_name('2231')] == 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######## GENRES ########\n",
    "\n",
    "def unique_genres(movies_genres):\n",
    "  genre_map = {}\n",
    "  for genres in movies_genres:\n",
    "    for genre in genres:\n",
    "      name = genre['name']\n",
    "      if not name in genre_map:\n",
    "        genre_map[name] = 0\n",
    "      genre_map[name]+=1\n",
    "  return pd.Series(genre_map).sort_values(ascending=False)\n",
    "\n",
    "def get_genres_feature_name(genre):\n",
    "  return genre['name'] + ' (genre)'\n",
    "\n",
    "movies_genres_ids = []\n",
    "movie_genres_names_by_id = {}\n",
    "for genres in movies.genres:\n",
    "  movie_genres_ids = []\n",
    "  for genre in genres:\n",
    "    genre_feature_name = get_genres_feature_name(genre)\n",
    "    movie_genres_ids.append(str(genre['id']))\n",
    "    movie_genres_names_by_id[str(genre['id'])] = genre_feature_name\n",
    "  movies_genres_ids.append(' '.join(movie_genres_ids))\n",
    "\n",
    "genre_vectorization = CountVectorizer()\n",
    "\n",
    "movie_vector = genre_vectorization.fit_transform(movies_genres_ids).toarray()\n",
    "genre_feature_ids = genre_vectorization.get_feature_names()\n",
    "\n",
    "def get_feature_names_by_ids(ids):\n",
    "  genre_feature_names = []\n",
    "\n",
    "  for id in ids:\n",
    "    genre_feature_names.append(movie_genres_names_by_id[id])\n",
    "  return genre_feature_names\n",
    "\n",
    "genre_feature_names = get_feature_names_by_ids(genre_feature_ids)\n",
    "\n",
    "genre_vector = pd.DataFrame(movie_vector, columns=genre_feature_names)\n",
    "genre_vector['id'] = movies.id\n",
    "\n",
    "features_to_normalize = [\n",
    "  'budget',\n",
    "  'popularity',\n",
    "  'vote_average',\n",
    "  'vote_count',\n",
    "  'revenue'\n",
    "]\n",
    "\n",
    "for feature in features_to_normalize:\n",
    "  feature_array = movies[feature].values.astype('float64').reshape(1, -1)\n",
    "  movies[feature + '_original'] =     movies[feature]\n",
    "  movies[feature] = normalize(feature_array, axis=1, norm='max')[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posters_test_dir = 'data/posters_test'\n",
    "posters_train_dir = 'data/posters_train'\n",
    "poster_test_filenames = os.listdir(posters_test_dir)\n",
    "poster_train_filenames = os.listdir(posters_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3355 images belonging to 2 classes.\n",
      "Found 1438 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        posters_train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(400, 600),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        posters_test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('data/posters_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importimport  matplotlib.pyplotmatplotl  as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
