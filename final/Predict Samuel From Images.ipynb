{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samuel_images_validation_dir = 'data/samuel_images_validation'\n",
    "samuel_images_train_dir = 'data/samuel_images_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.SeparableConv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(200, 300, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.SeparableConv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "separable_conv2d_1 (Separabl (None, 198, 298, 32)      155       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 99, 149, 32)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_2 (Separabl (None, 97, 147, 64)       2400      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 48, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_3 (Separabl (None, 46, 71, 128)       8896      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 23, 35, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 103040)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               52756992  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 52,768,956\n",
      "Trainable params: 52,768,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1440 images belonging to 2 classes.\n",
      "Found 714 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        samuel_images_train_dir,\n",
    "        target_size=(200, 300),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        samuel_images_validation_dir,\n",
    "        target_size=(200, 300),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "  3/100 [..............................] - ETA: 5:31 - loss: 0.6953 - acc: 0.5500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/PIL/Image.py:918: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 263s 3s/step - loss: 0.6896 - acc: 0.5420 - val_loss: 0.6885 - val_acc: 0.5181\n",
      "Epoch 2/60\n",
      "100/100 [==============================] - 261s 3s/step - loss: 0.6712 - acc: 0.5910 - val_loss: 0.6391 - val_acc: 0.6429\n",
      "Epoch 3/60\n",
      "100/100 [==============================] - 263s 3s/step - loss: 0.6689 - acc: 0.5855 - val_loss: 0.6454 - val_acc: 0.6449\n",
      "Epoch 4/60\n",
      "100/100 [==============================] - 260s 3s/step - loss: 0.6549 - acc: 0.6195 - val_loss: 0.6277 - val_acc: 0.6590\n",
      "Epoch 5/60\n",
      "100/100 [==============================] - 262s 3s/step - loss: 0.6540 - acc: 0.6170 - val_loss: 0.6268 - val_acc: 0.6358\n",
      "Epoch 6/60\n",
      "100/100 [==============================] - 265s 3s/step - loss: 0.6537 - acc: 0.6115 - val_loss: 0.6173 - val_acc: 0.6690\n",
      "Epoch 7/60\n",
      "100/100 [==============================] - 266s 3s/step - loss: 0.6529 - acc: 0.6150 - val_loss: 0.6879 - val_acc: 0.5744\n",
      "Epoch 8/60\n",
      "100/100 [==============================] - 278s 3s/step - loss: 0.6456 - acc: 0.6195 - val_loss: 0.6038 - val_acc: 0.6781\n",
      "Epoch 9/60\n",
      "100/100 [==============================] - 259s 3s/step - loss: 0.6448 - acc: 0.6260 - val_loss: 0.6005 - val_acc: 0.6720\n",
      "Epoch 10/60\n",
      "100/100 [==============================] - 257s 3s/step - loss: 0.6451 - acc: 0.6140 - val_loss: 0.6044 - val_acc: 0.6781\n",
      "Epoch 11/60\n",
      "100/100 [==============================] - 261s 3s/step - loss: 0.6486 - acc: 0.6175 - val_loss: 0.5873 - val_acc: 0.6861\n",
      "Epoch 12/60\n",
      "100/100 [==============================] - 259s 3s/step - loss: 0.6473 - acc: 0.6195 - val_loss: 0.5897 - val_acc: 0.6841\n",
      "Epoch 13/60\n",
      "100/100 [==============================] - 261s 3s/step - loss: 0.6498 - acc: 0.6185 - val_loss: 0.6137 - val_acc: 0.6539\n",
      "Epoch 14/60\n",
      "100/100 [==============================] - 256s 3s/step - loss: 0.6461 - acc: 0.6115 - val_loss: 0.5803 - val_acc: 0.7093\n",
      "Epoch 15/60\n",
      "100/100 [==============================] - 260s 3s/step - loss: 0.6393 - acc: 0.6445 - val_loss: 0.6110 - val_acc: 0.6620\n",
      "Epoch 16/60\n",
      "100/100 [==============================] - 262s 3s/step - loss: 0.6388 - acc: 0.6380 - val_loss: 0.5837 - val_acc: 0.6962\n",
      "Epoch 17/60\n",
      "100/100 [==============================] - 262s 3s/step - loss: 0.6392 - acc: 0.6270 - val_loss: 0.5948 - val_acc: 0.6962\n",
      "Epoch 18/60\n",
      "100/100 [==============================] - 261s 3s/step - loss: 0.6253 - acc: 0.6520 - val_loss: 0.5956 - val_acc: 0.6871\n",
      "Epoch 19/60\n",
      "100/100 [==============================] - 251s 3s/step - loss: 0.6284 - acc: 0.6535 - val_loss: 0.5610 - val_acc: 0.7254\n",
      "Epoch 20/60\n",
      "100/100 [==============================] - 258s 3s/step - loss: 0.6314 - acc: 0.6510 - val_loss: 0.5756 - val_acc: 0.6881\n",
      "Epoch 21/60\n",
      "100/100 [==============================] - 262s 3s/step - loss: 0.6280 - acc: 0.6525 - val_loss: 0.5725 - val_acc: 0.7193\n",
      "Epoch 22/60\n",
      "100/100 [==============================] - 259s 3s/step - loss: 0.6293 - acc: 0.6520 - val_loss: 0.5735 - val_acc: 0.7042\n",
      "Epoch 23/60\n",
      "100/100 [==============================] - 258s 3s/step - loss: 0.6174 - acc: 0.6505 - val_loss: 0.5735 - val_acc: 0.7032\n",
      "Epoch 24/60\n",
      "100/100 [==============================] - 261s 3s/step - loss: 0.6195 - acc: 0.6425 - val_loss: 0.6501 - val_acc: 0.6318\n",
      "Epoch 25/60\n",
      "100/100 [==============================] - 267s 3s/step - loss: 0.6217 - acc: 0.6565 - val_loss: 0.5832 - val_acc: 0.7012\n",
      "Epoch 26/60\n",
      "100/100 [==============================] - 280s 3s/step - loss: 0.6234 - acc: 0.6495 - val_loss: 0.5547 - val_acc: 0.7344\n",
      "Epoch 27/60\n",
      "100/100 [==============================] - 265s 3s/step - loss: 0.6236 - acc: 0.6485 - val_loss: 0.5530 - val_acc: 0.7274\n",
      "Epoch 28/60\n",
      "100/100 [==============================] - 267s 3s/step - loss: 0.6196 - acc: 0.6545 - val_loss: 0.5716 - val_acc: 0.7082\n",
      "Epoch 29/60\n",
      "100/100 [==============================] - 302s 3s/step - loss: 0.6216 - acc: 0.6565 - val_loss: 0.5580 - val_acc: 0.7274\n",
      "Epoch 30/60\n",
      "100/100 [==============================] - 280s 3s/step - loss: 0.6062 - acc: 0.6685 - val_loss: 0.5509 - val_acc: 0.7304\n",
      "Epoch 31/60\n",
      "100/100 [==============================] - 276s 3s/step - loss: 0.6124 - acc: 0.6675 - val_loss: 0.5483 - val_acc: 0.7223\n",
      "Epoch 32/60\n",
      "100/100 [==============================] - 270s 3s/step - loss: 0.6054 - acc: 0.6645 - val_loss: 0.5472 - val_acc: 0.7264\n",
      "Epoch 33/60\n",
      "100/100 [==============================] - 260s 3s/step - loss: 0.6079 - acc: 0.6650 - val_loss: 0.5373 - val_acc: 0.7314\n",
      "Epoch 34/60\n",
      "100/100 [==============================] - 270s 3s/step - loss: 0.5956 - acc: 0.6770 - val_loss: 0.5455 - val_acc: 0.7173\n",
      "Epoch 35/60\n",
      "100/100 [==============================] - 338s 3s/step - loss: 0.6056 - acc: 0.6565 - val_loss: 0.5922 - val_acc: 0.6932\n",
      "Epoch 36/60\n",
      "100/100 [==============================] - 351s 4s/step - loss: 0.6010 - acc: 0.6835 - val_loss: 0.5673 - val_acc: 0.7113\n",
      "Epoch 37/60\n",
      "100/100 [==============================] - 335s 3s/step - loss: 0.6002 - acc: 0.6765 - val_loss: 0.5410 - val_acc: 0.7213\n",
      "Epoch 38/60\n",
      "100/100 [==============================] - 360s 4s/step - loss: 0.5890 - acc: 0.6835 - val_loss: 0.5578 - val_acc: 0.7183\n",
      "Epoch 39/60\n",
      "100/100 [==============================] - 362s 4s/step - loss: 0.5905 - acc: 0.6910 - val_loss: 0.5439 - val_acc: 0.7264\n",
      "Epoch 40/60\n",
      "100/100 [==============================] - 270s 3s/step - loss: 0.5928 - acc: 0.6815 - val_loss: 0.5543 - val_acc: 0.7143\n",
      "Epoch 41/60\n",
      "100/100 [==============================] - 255s 3s/step - loss: 0.6024 - acc: 0.6670 - val_loss: 0.5641 - val_acc: 0.7022\n",
      "Epoch 42/60\n",
      "100/100 [==============================] - 262s 3s/step - loss: 0.5981 - acc: 0.6795 - val_loss: 0.5465 - val_acc: 0.7314\n",
      "Epoch 43/60\n",
      "100/100 [==============================] - 260s 3s/step - loss: 0.5814 - acc: 0.6990 - val_loss: 0.5476 - val_acc: 0.7183\n",
      "Epoch 44/60\n",
      "100/100 [==============================] - 267s 3s/step - loss: 0.5886 - acc: 0.6815 - val_loss: 0.5967 - val_acc: 0.6831\n",
      "Epoch 45/60\n",
      "100/100 [==============================] - 267s 3s/step - loss: 0.5880 - acc: 0.6710 - val_loss: 0.5666 - val_acc: 0.7143\n",
      "Epoch 46/60\n",
      "100/100 [==============================] - 270s 3s/step - loss: 0.5932 - acc: 0.6685 - val_loss: 0.5485 - val_acc: 0.7173\n",
      "Epoch 47/60\n",
      "100/100 [==============================] - 287s 3s/step - loss: 0.5770 - acc: 0.6855 - val_loss: 0.5249 - val_acc: 0.7314\n",
      "Epoch 48/60\n",
      "100/100 [==============================] - 275s 3s/step - loss: 0.5881 - acc: 0.6865 - val_loss: 0.5180 - val_acc: 0.7485\n",
      "Epoch 49/60\n",
      "100/100 [==============================] - 269s 3s/step - loss: 0.5816 - acc: 0.6970 - val_loss: 0.5559 - val_acc: 0.7284\n",
      "Epoch 50/60\n",
      "100/100 [==============================] - 223s 2s/step - loss: 0.5811 - acc: 0.6965 - val_loss: 0.5589 - val_acc: 0.7223\n",
      "Epoch 51/60\n",
      " 74/100 [=====================>........] - ETA: 40s - loss: 0.5839 - acc: 0.6899"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=60,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'data/samuel_images_model_{}.h5'.format(time.time())\n",
    "model.save(model_file)\n",
    "\n",
    "print('saved to {}'.format(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot The ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samuel_images_test_dir = 'data/samuel_images_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        samuel_images_test_dir,\n",
    "        target_size=(200, 300),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate_generator(test_generator)\n",
    "print('loss={}, acc={}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display predicted items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_results = pd.DataFrame({\n",
    "    'prediction': predictions.flatten(),\n",
    "    'filename': test_generator.filenames\n",
    "}) \\\n",
    ".sort_values('prediction', ascending=False) \\\n",
    ".head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [os.path.join(samuel_images_test_dir, fname) for fname in test_image_results.filename]\n",
    "\n",
    "imgs = [image.load_img(fname, target_size=(200,300)) for fname in fnames]\n",
    "show_images(imgs, 5, test_image_results.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Grid Search to use all cores https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
