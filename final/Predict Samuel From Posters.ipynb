{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load load-movies.py\n",
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv('./data/tmdb_5000_movies.csv')\n",
    "credits = pd.read_csv('./data/tmdb_5000_credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
     ]
    }
   ],
   "source": [
    "# %load clean-movies.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# merge two datasets\n",
    "movies = pd.merge(left=movies, right=credits, left_on='id', right_on='movie_id')\n",
    "\n",
    "# turn json into python objects\n",
    "json_columns = ['genres', 'keywords', 'production_countries', 'spoken_languages', 'cast', 'crew']\n",
    "for column in json_columns:\n",
    "  movies[column] = movies[column].apply(json.loads, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "######## Actors ########\n",
    "\n",
    "# create functions that will help with extracting actor information\n",
    "def actor_to_id_string(actor):\n",
    "  return '{} - {}'.format(actor['name'], actor['id'])\n",
    "\n",
    "def cast_to_actors(cast):\n",
    "  actors = []\n",
    "  for person in cast:\n",
    "    actors.append(actor_to_id_string(person))\n",
    "  return actors\n",
    "\n",
    "# Create a new column (array) from cast\n",
    "movies.actors = movies.cast.apply(cast_to_actors)\n",
    "\n",
    "# Create a list of all actors\n",
    "all_actors = []\n",
    "for actors in movies.actors:\n",
    "  for actor in actors:\n",
    "    all_actors.append(actor)\n",
    "actors = pd.Series(all_actors)\n",
    "unique_actors_count = pd.crosstab(actors, columns='count')\n",
    "\n",
    "# Create function to determine if actor has been in more than 3 films\n",
    "actors_string_id_by_id = {}\n",
    "actor_counts_by_string_id = {}\n",
    "unique_actors_count_dict = unique_actors_count.to_dict()['count']\n",
    "\n",
    "def actor_has_more_than_x_movies(actor, number_of_movies = 3):\n",
    "    string_id = str(actor['id'])\n",
    "\n",
    "    # First, get the unique id created for the actor\n",
    "    actor_string_id = ''\n",
    "    if string_id in actors_string_id_by_id:\n",
    "        actor_string_id = actors_string_id_by_id[string_id]\n",
    "    else:\n",
    "        actor_string_id = actor_to_id_string(actor)\n",
    "        actors_string_id_by_id[string_id] = actor_string_id\n",
    "    \n",
    "    # Now let's see how many movies this actor has played in\n",
    "    actor_count = unique_actors_count_dict[actor_string_id]\n",
    "    return actor_count > number_of_movies\n",
    "\n",
    "# Create a list of all actors that have been in more than 3 movies\n",
    "movies_actors_ids = []\n",
    "for actors in movies.cast:\n",
    "  movie_actors_ids = []\n",
    "  for actor in actors:\n",
    "    if (not actor_has_more_than_x_movies(actor, 3)):\n",
    "      continue\n",
    "    movie_actors_ids.append(str(actor['id']))\n",
    "  movies_actors_ids.append(' '.join(movie_actors_ids))\n",
    "\n",
    "# Vectorize the list of actors\n",
    "def get_actor_feature_name(id):\n",
    "  return actors_string_id_by_id[id] + ' (actor)'\n",
    "vectorizer = CountVectorizer()\n",
    "movie_vector = vectorizer.fit_transform(movies_actors_ids).toarray()\n",
    "\n",
    "actor_feature_ids = vectorizer.get_feature_names()\n",
    "actor_feature_names = []\n",
    "for id in actor_feature_ids:\n",
    "  actor_name = get_actor_feature_name(id)\n",
    "  actor_feature_names.append(actor_name)\n",
    "\n",
    "actor_vector_works = pd.DataFrame(movie_vector, columns=actor_feature_names)\n",
    "actor_vector_works['id'] = movies.id\n",
    "\n",
    "# Create the label for whether Samuel L Jackson was in the film (samuel)\n",
    "movies['samuel'] = actor_vector_works[get_actor_feature_name('2231')] == 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######## GENRES ########\n",
    "\n",
    "def unique_genres(movies_genres):\n",
    "  genre_map = {}\n",
    "  for genres in movies_genres:\n",
    "    for genre in genres:\n",
    "      name = genre['name']\n",
    "      if not name in genre_map:\n",
    "        genre_map[name] = 0\n",
    "      genre_map[name]+=1\n",
    "  return pd.Series(genre_map).sort_values(ascending=False)\n",
    "\n",
    "def get_genres_feature_name(genre):\n",
    "  return genre['name'] + ' (genre)'\n",
    "\n",
    "movies_genres_ids = []\n",
    "movie_genres_names_by_id = {}\n",
    "for genres in movies.genres:\n",
    "  movie_genres_ids = []\n",
    "  for genre in genres:\n",
    "    genre_feature_name = get_genres_feature_name(genre)\n",
    "    movie_genres_ids.append(str(genre['id']))\n",
    "    movie_genres_names_by_id[str(genre['id'])] = genre_feature_name\n",
    "  movies_genres_ids.append(' '.join(movie_genres_ids))\n",
    "\n",
    "genre_vectorization = CountVectorizer()\n",
    "\n",
    "movie_vector = genre_vectorization.fit_transform(movies_genres_ids).toarray()\n",
    "genre_feature_ids = genre_vectorization.get_feature_names()\n",
    "\n",
    "def get_feature_names_by_ids(ids):\n",
    "  genre_feature_names = []\n",
    "\n",
    "  for id in ids:\n",
    "    genre_feature_names.append(movie_genres_names_by_id[id])\n",
    "  return genre_feature_names\n",
    "\n",
    "genre_feature_names = get_feature_names_by_ids(genre_feature_ids)\n",
    "\n",
    "genre_vector = pd.DataFrame(movie_vector, columns=genre_feature_names)\n",
    "genre_vector['id'] = movies.id\n",
    "\n",
    "features_to_normalize = [\n",
    "  'budget',\n",
    "  'popularity',\n",
    "  'vote_average',\n",
    "  'vote_count',\n",
    "  'revenue'\n",
    "]\n",
    "\n",
    "for feature in features_to_normalize:\n",
    "  feature_array = movies[feature].values.astype('float64').reshape(1, -1)\n",
    "  movies[feature + '_original'] =     movies[feature]\n",
    "  movies[feature] = normalize(feature_array, axis=1, norm='max')[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "posters_test_dir = 'data/posters_test'\n",
    "posters_train_dir = 'data/posters_train'\n",
    "poster_test_filenames = os.listdir(posters_test_dir)\n",
    "poster_train_filenames = os.listdir(posters_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(400, 600, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3355 images belonging to 2 classes.\n",
      "Found 1438 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        posters_train_dir,\n",
    "        target_size=(400, 600),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        posters_test_dir,\n",
    "        target_size=(400, 600),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 729s 7s/step - loss: 0.1055 - acc: 0.9785 - val_loss: 0.0850 - val_acc: 0.9860\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 718s 7s/step - loss: 0.0786 - acc: 0.9895 - val_loss: 0.0800 - val_acc: 0.9860\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 718s 7s/step - loss: 0.0968 - acc: 0.9835 - val_loss: 0.0754 - val_acc: 0.9860\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 715s 7s/step - loss: 0.0851 - acc: 0.9840 - val_loss: 0.0947 - val_acc: 0.9860\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 714s 7s/step - loss: 0.0719 - acc: 0.9885 - val_loss: 0.0830 - val_acc: 0.9860\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 718s 7s/step - loss: 0.0761 - acc: 0.9870 - val_loss: 0.0744 - val_acc: 0.9860\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 714s 7s/step - loss: 0.0827 - acc: 0.9855 - val_loss: 0.0912 - val_acc: 0.9860\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 714s 7s/step - loss: 0.0746 - acc: 0.9835 - val_loss: 0.0768 - val_acc: 0.9860\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 716s 7s/step - loss: 0.0543 - acc: 0.9890 - val_loss: 0.0842 - val_acc: 0.9860\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 718s 7s/step - loss: 0.0703 - acc: 0.9840 - val_loss: 0.0769 - val_acc: 0.9860\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 714s 7s/step - loss: 0.0551 - acc: 0.9870 - val_loss: 0.0801 - val_acc: 0.9860\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 718s 7s/step - loss: 0.0530 - acc: 0.9865 - val_loss: 0.0973 - val_acc: 0.9860\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 714s 7s/step - loss: 0.0530 - acc: 0.9860 - val_loss: 0.1242 - val_acc: 0.9860\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 716s 7s/step - loss: 0.0551 - acc: 0.9855 - val_loss: 0.1004 - val_acc: 0.9860\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 715s 7s/step - loss: 0.0394 - acc: 0.9865 - val_loss: 0.0825 - val_acc: 0.9860\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 716s 7s/step - loss: 0.0457 - acc: 0.9840 - val_loss: 0.1025 - val_acc: 0.9860\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 715s 7s/step - loss: 0.0223 - acc: 0.9935 - val_loss: 0.0989 - val_acc: 0.9860\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 715s 7s/step - loss: 0.0390 - acc: 0.9845 - val_loss: 0.1164 - val_acc: 0.9860\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 715s 7s/step - loss: 0.0399 - acc: 0.9900 - val_loss: 0.0960 - val_acc: 0.9860\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 719s 7s/step - loss: 0.0350 - acc: 0.9845 - val_loss: 0.1140 - val_acc: 0.9860\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 717s 7s/step - loss: 0.0205 - acc: 0.9930 - val_loss: 0.1820 - val_acc: 0.9860\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 716s 7s/step - loss: 0.0290 - acc: 0.9865 - val_loss: 0.1754 - val_acc: 0.9860\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 715s 7s/step - loss: 0.0353 - acc: 0.9880 - val_loss: 0.1281 - val_acc: 0.9860\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 716s 7s/step - loss: 0.0161 - acc: 0.9920 - val_loss: 0.1518 - val_acc: 0.9860\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 716s 7s/step - loss: 0.0173 - acc: 0.9925 - val_loss: 0.1498 - val_acc: 0.9860\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 717s 7s/step - loss: 0.0223 - acc: 0.9945 - val_loss: 0.1659 - val_acc: 0.9860\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 718s 7s/step - loss: 0.0157 - acc: 0.9950 - val_loss: 0.1443 - val_acc: 0.9850\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 714s 7s/step - loss: 0.0123 - acc: 0.9965 - val_loss: 0.1716 - val_acc: 0.9860\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 716s 7s/step - loss: 0.0169 - acc: 0.9950 - val_loss: 0.2098 - val_acc: 0.9860\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 715s 7s/step - loss: 0.0094 - acc: 0.9975 - val_loss: 0.1859 - val_acc: 0.9860\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data/posters_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
