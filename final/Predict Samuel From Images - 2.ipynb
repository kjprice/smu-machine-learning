{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samuel_images_validation_dir = 'data/samuel_images_validation'\n",
    "samuel_images_train_dir = 'data/samuel_images_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.SeparableConv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(224, 224, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.SeparableConv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.SeparableConv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "separable_conv2d_1 (Separabl (None, 222, 222, 32)      155       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_2 (Separabl (None, 109, 109, 64)      2400      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_3 (Separabl (None, 52, 52, 128)       8896      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_4 (Separabl (None, 24, 24, 128)       17664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               9437696   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 9,467,324\n",
      "Trainable params: 9,467,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1524 images belonging to 2 classes.\n",
      "Found 191 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        samuel_images_train_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=60,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        samuel_images_validation_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=60,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 301s 3s/step - loss: 0.6892 - acc: 0.5432 - val_loss: 0.6752 - val_acc: 0.6277\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 299s 3s/step - loss: 0.6675 - acc: 0.5992 - val_loss: 0.6547 - val_acc: 0.6347\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 299s 3s/step - loss: 0.6597 - acc: 0.6053 - val_loss: 0.6307 - val_acc: 0.6750\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 297s 3s/step - loss: 0.6505 - acc: 0.6326 - val_loss: 0.6132 - val_acc: 0.6828\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 296s 3s/step - loss: 0.6460 - acc: 0.6354 - val_loss: 0.5999 - val_acc: 0.7065\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 296s 3s/step - loss: 0.6413 - acc: 0.6432 - val_loss: 0.5965 - val_acc: 0.7396\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 294s 3s/step - loss: 0.6412 - acc: 0.6436 - val_loss: 0.5914 - val_acc: 0.6857\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 298s 3s/step - loss: 0.6347 - acc: 0.6450 - val_loss: 0.5808 - val_acc: 0.7276\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 298s 3s/step - loss: 0.6327 - acc: 0.6492 - val_loss: 0.5803 - val_acc: 0.7338\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 296s 3s/step - loss: 0.6313 - acc: 0.6536 - val_loss: 0.5771 - val_acc: 0.7376\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 297s 3s/step - loss: 0.6275 - acc: 0.6560 - val_loss: 0.5712 - val_acc: 0.7284\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 296s 3s/step - loss: 0.6301 - acc: 0.6518 - val_loss: 0.5689 - val_acc: 0.7313\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 295s 3s/step - loss: 0.6248 - acc: 0.6594 - val_loss: 0.5637 - val_acc: 0.7326\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 293s 3s/step - loss: 0.6250 - acc: 0.6570 - val_loss: 0.5871 - val_acc: 0.6762\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 297s 3s/step - loss: 0.6248 - acc: 0.6571 - val_loss: 0.5591 - val_acc: 0.7334\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 297s 3s/step - loss: 0.6196 - acc: 0.6612 - val_loss: 0.5613 - val_acc: 0.7260\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 295s 3s/step - loss: 0.6161 - acc: 0.6642 - val_loss: 0.5545 - val_acc: 0.7380\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 295s 3s/step - loss: 0.6084 - acc: 0.6786 - val_loss: 0.5570 - val_acc: 0.7114\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 297s 3s/step - loss: 0.6111 - acc: 0.6768 - val_loss: 0.5567 - val_acc: 0.7326\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 293s 3s/step - loss: 0.6121 - acc: 0.6732 - val_loss: 0.5544 - val_acc: 0.7309\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 300s 3s/step - loss: 0.6059 - acc: 0.6731 - val_loss: 0.5780 - val_acc: 0.7061\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 300s 3s/step - loss: 0.6110 - acc: 0.6750 - val_loss: 0.5457 - val_acc: 0.7483\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 295s 3s/step - loss: 0.6049 - acc: 0.6757 - val_loss: 0.5412 - val_acc: 0.7425\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 297s 3s/step - loss: 0.6039 - acc: 0.6755 - val_loss: 0.5493 - val_acc: 0.7587\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 296s 3s/step - loss: 0.5982 - acc: 0.6857 - val_loss: 0.5674 - val_acc: 0.7069\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 294s 3s/step - loss: 0.5978 - acc: 0.6824 - val_loss: 0.5327 - val_acc: 0.7512\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 296s 3s/step - loss: 0.5927 - acc: 0.6858 - val_loss: 0.5286 - val_acc: 0.7546\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 304s 3s/step - loss: 0.5991 - acc: 0.6848 - val_loss: 0.5304 - val_acc: 0.7699\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 301s 3s/step - loss: 0.5913 - acc: 0.6857 - val_loss: 0.5277 - val_acc: 0.7533\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 298s 3s/step - loss: 0.5875 - acc: 0.6986 - val_loss: 0.5283 - val_acc: 0.7720\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 296s 3s/step - loss: 0.5900 - acc: 0.6896 - val_loss: 0.5235 - val_acc: 0.7629\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 298s 3s/step - loss: 0.5937 - acc: 0.6861 - val_loss: 0.5223 - val_acc: 0.7707\n",
      "Epoch 33/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.5868 - acc: 0.6895"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'data/saved_models_local/samuel_images_model_{}.h5'.format(time.time())\n",
    "model.save(model_file)\n",
    "\n",
    "print('saved to {}'.format(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot The ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samuel_images_test_dir = 'data/samuel_images_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        samuel_images_test_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=60,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate_generator(test_generator)\n",
    "print('loss={}, acc={}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display predicted items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_results = pd.DataFrame({\n",
    "    'prediction': predictions.flatten(),\n",
    "    'filename': test_generator.filenames\n",
    "}) \\\n",
    ".sort_values('prediction', ascending=False) \\\n",
    ".head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [os.path.join(samuel_images_test_dir, fname) for fname in test_image_results.filename]\n",
    "\n",
    "imgs = [image.load_img(fname, target_size=(224,224)) for fname in fnames]\n",
    "show_images(imgs, 5, test_image_results.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Grid Search to use all cores https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
